{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BAgFh9OI05C"
   },
   "source": [
    "#Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jqxdm0XjMsf"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5S1K2vtdI24H"
   },
   "source": [
    "#Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JkOUdVioleOP"
   },
   "outputs": [],
   "source": [
    "#For the system\n",
    "import os\n",
    "\n",
    "#Manage of time\n",
    "from datetime import datetime, timedelta\n",
    "from pytz import timezone\n",
    "import time\n",
    "import re\n",
    "\n",
    "#Manage of files\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "#scrap\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl.workbook import Workbook\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3y5bz5iksCz"
   },
   "source": [
    "# Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGv2pyl01c7D"
   },
   "outputs": [],
   "source": [
    "headers_0 = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64;x64; rv:66.0) Gecko/20100101 Firefox/66.0\",\n",
    "           \"Accept-Encoding\":\"gzip, deflate\",\n",
    "           \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "           \"DNT\":\"1\",\n",
    "           \"Connection\":\"close\",\n",
    "           \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "\n",
    "headers_1 = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5)AppleWebKit/605.1.15 (KHTML, like Gecko)Version/12.1.1 Safari/605.1.15\",\n",
    "           \"Accept-Encoding\":\"gzip, deflate\",\n",
    "           \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "           \"DNT\":\"1\",\n",
    "           \"Connection\":\"close\",\n",
    "           \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "\n",
    "headers_2 = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36\",\n",
    "           \"Accept-Encoding\":\"gzip, deflate\",\n",
    "           \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "           \"DNT\":\"1\",\n",
    "           \"Connection\":\"close\",\n",
    "           \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "\n",
    "headers_tuple = (headers_1, headers_1, headers_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wnD2GaNJdu3"
   },
   "outputs": [],
   "source": [
    "# # def headers_walk(headers_tuple=headers_tuple, spot):\n",
    "# header_count = 0\n",
    "# headers_tuple[header_count]\n",
    "# header_count += 1\n",
    "# if header_count > 3:\n",
    "#     header_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rc_h035RI4YO"
   },
   "source": [
    "# Extract soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vEctUyZylWEZ"
   },
   "outputs": [],
   "source": [
    "def extract_soup(url, header=0, preview=True):\n",
    "    headers = headers_tuple[header]\n",
    "    response = requests.get(url, headers=headers)\n",
    "    status = response.status_code\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    if preview==True:\n",
    "        print(soup.prettify())\n",
    "\n",
    "    return soup, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hFbAeqp4O1Q"
   },
   "outputs": [],
   "source": [
    "    # test_url = 'https://www.amazon.com.mx/gp/bestsellers/musical-instruments/ref=zg_bs_nav_0'\n",
    "    # test_soup, test_status = extract_soup(test_url, False)\n",
    "    # print(test_status)\n",
    "    # print(type(test_status))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9LFdhJaI7c2"
   },
   "source": [
    "#Top amazon boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3HmEasZNlXC7"
   },
   "outputs": [],
   "source": [
    "def top_amazon_boxes(soup):\n",
    "    boxes = soup.find_all('div', attrs={'class':\"a-section a-spacing-none aok-relative\"})\n",
    "\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72ieGkSdI9jv"
   },
   "source": [
    "#Centered String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCG4GpMGkmNp"
   },
   "outputs": [],
   "source": [
    "def centered_string(string):\n",
    "    rest = 21 - len(string)\n",
    "    n_spaces = int(rest/2)\n",
    "\n",
    "    string_spaces = ' ' * n_spaces\n",
    "    \n",
    "    centered_string = string_spaces + string + string_spaces\n",
    "\n",
    "    if len(centered_string)<21:\n",
    "        centered_string = ' ' + centered_string\n",
    "\n",
    "    return centered_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3P4MlltBJBDi"
   },
   "source": [
    "#Update CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3xAa-RvItTg"
   },
   "outputs": [],
   "source": [
    "def update_csv(file, rows, sep=','):\n",
    "    with open(file, \"a\", encoding=\"utf-8\") as csv_file:\n",
    "        csv_file = csv.writer(csv_file, delimiter=sep)\n",
    "        for row in rows:\n",
    "            csv_file.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sy2H16PYbwxC"
   },
   "source": [
    "# Product ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8oZm1LOb0N7"
   },
   "outputs": [],
   "source": [
    "def product_id(complete_product_link):\n",
    "    regex_a = '(?<=dp\\/)(.+)(?=\\/ref)'\n",
    "    regex_b = '(?<=dp\\/)(.+)(?=\\?)'\n",
    "\n",
    "    product_link_a = re.search(regex_a, complete_product_link)\n",
    "    if product_link_a is None:\n",
    "\n",
    "        product_link_b = re.search(regex_b, complete_product_link)\n",
    "        if product_link_b is None:\n",
    "            product_id = complete_product_link\n",
    "        else:\n",
    "            product_id = product_link_b.group()\n",
    "    else:\n",
    "        product_id = product_link_a.group()\n",
    "    \n",
    "    return product_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykFkvZlocavO"
   },
   "source": [
    "# Image ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5Hg4MghcfUx"
   },
   "outputs": [],
   "source": [
    "def image_id(complete_image_link):\n",
    "    regex_a = '(?<=I\\/)(.*)(?=\\.jpg)'\n",
    "    image_link_a = re.search(regex_a, complete_image_link)\n",
    "\n",
    "    if not image_link_a:\n",
    "        image_id = complete_image_link\n",
    "    else:\n",
    "        image_id = image_link_a.group()\n",
    "    \n",
    "    return image_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JKvH854JGtB"
   },
   "source": [
    "#Scrap boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1FRWXKFZlcUx"
   },
   "outputs": [],
   "source": [
    "def scrap_boxes(boxes, domain, dict_to_rows=False):\n",
    "    \n",
    "    ranks = [None]*50\n",
    "    product_names = [None]*50\n",
    "    image_urls = [None]*50\n",
    "    product_links = [None]*50\n",
    "    star_ratings = [None]*50\n",
    "    reviews = [None]*50\n",
    "    authors_companies = [None]*50\n",
    "    editions_consoles = [None]*50\n",
    "    min_prices = [None]*50\n",
    "    max_prices = [None]*50\n",
    "    time_log = [None]*50\n",
    "\n",
    "    amz_mx_url = f'https://www.amazon.com.{domain}'\n",
    "    \n",
    "    n_box = 0\n",
    "    for box in boxes:\n",
    "        MX__COL_tz = 'America/Mexico_City'\n",
    "        timezone_MXCOL = timezone(MX__COL_tz) \n",
    "        datetime_box = datetime.now(timezone_MXCOL)\n",
    "        time_log[n_box] = datetime_box.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "        rank_box = box.find_all('span', attrs={'class':'zg-badge-text'})\n",
    "        products_and_image_box = box.find_all('div', attrs={'class' : 'a-section a-spacing-small'})\n",
    "        product_links_box = box.find_all('a', attrs={'class' : 'a-link-normal'})\n",
    "        star_ratings_box = box.find_all('span', attrs={'class' : 'a-icon-alt'})\n",
    "        reviews_box = box.find_all('a', attrs={'class' : 'a-size-small a-link-normal'})\n",
    "        authors_company_box = box.find_all('span', attrs={'class' : 'a-size-small a-color-base'})\n",
    "        editions_console_box = box.find_all('span', attrs={'class' : 'a-size-small a-color-secondary'})\n",
    "        prices_box = box.find_all('span', attrs={'class' : \"p13n-sc-price\"})\n",
    "        \n",
    "        \n",
    "        ranks[n_box] = int(rank_box[0].get_text()[1:])\n",
    "\n",
    "        #In case the element was removed (yes, it happens)\n",
    "        try:\n",
    "            product_names[n_box] = products_and_image_box[0].img.get('alt')\n",
    "        except: pass\n",
    "\n",
    "        try:\n",
    "            complete_product_link = product_links_box[0].get('href')\n",
    "            product_links[n_box] = product_id(complete_product_link)\n",
    "        except: pass\n",
    "\n",
    "        try:\n",
    "            complete_image_id = products_and_image_box[0].img.get('src')\n",
    "            image_urls[n_box] = image_id(complete_image_id)\n",
    "        except: pass\n",
    "\n",
    "        try:\n",
    "            if domain == 'mx':\n",
    "                star_ratings[n_box] = float(star_ratings_box[0].get_text()[:3])\n",
    "                reviews[n_box] = int(reviews_box[0].get_text().replace(',',''))\n",
    "\n",
    "            elif domain == 'br':\n",
    "                star_ratings[n_box] = float(star_ratings_box[0].get_text()[:3].replace(',','.'))\n",
    "                reviews[n_box] = int(reviews_box[0].get_text().replace('.',''))\n",
    "        except: pass\n",
    "        #Individual cases\n",
    "        try:\n",
    "            authors_companies[n_box] = authors_company_box[0].get_text()\n",
    "        except:pass\n",
    "        \n",
    "        try:\n",
    "            editions_consoles[n_box] = editions_console_box[0].get_text()\n",
    "        except:pass\n",
    "\n",
    "        #Courrencies\n",
    "        if domain == 'mx':\n",
    "            coin_symbol = 1\n",
    "        elif domain == 'br':\n",
    "            coin_symbol = 2\n",
    "\n",
    "        try:\n",
    "            if domain == 'mx':\n",
    "                min_prices[n_box] = float(prices_box[0].get_text()[coin_symbol:].replace(',',''))\n",
    "            elif domain == 'br':\n",
    "                min_prices[n_box] = float(prices_box[0].get_text()[coin_symbol:].replace('.','').replace(',','.'))\n",
    "        except: pass\n",
    "\n",
    "        try:    \n",
    "            if domain == 'mx':\n",
    "                max_prices[n_box] = float(prices_box[1].get_text()[coin_symbol:].replace(',',''))\n",
    "            elif domain == 'br':\n",
    "                max_prices[n_box] = float(prices_box[1].get_text()[coin_symbol:].replace('.','').replace(',','.'))\n",
    "        except: pass\n",
    "\n",
    "        n_box = n_box + 1 \n",
    "\n",
    "    # Dictionary\n",
    "    boxes_dict = {\n",
    "    'time' : time_log,\n",
    "    \"Rank\" : ranks,\n",
    "    \"Product Names\": product_names,\n",
    "    \"Product ID\": product_links,\n",
    "    \"Image ID\": image_urls,\n",
    "    \"Stars\": star_ratings,\n",
    "    \"Reviews\": reviews,\n",
    "    \"Authors/Company\": authors_companies,\n",
    "    \"Edition/Console\": editions_consoles,\n",
    "    \"Price_std_or_min\" : min_prices,\n",
    "    \"Max_prices\" : max_prices\n",
    "    }\n",
    "\n",
    "    if dict_to_rows == True:\n",
    "        dict_rows = [None]*50\n",
    "        for n in range(len(dict_rows)):\n",
    "            dict_rows[n] = [\n",
    "                boxes_dict[\"time\"][n],\n",
    "                boxes_dict[\"Rank\"][n],\n",
    "                boxes_dict[\"Product Names\"][n],\n",
    "                boxes_dict[\"Product ID\"][n],\n",
    "                boxes_dict[\"Image ID\"][n],\n",
    "                boxes_dict[\"Stars\"][n],\n",
    "                boxes_dict[\"Reviews\"][n],\n",
    "                boxes_dict[\"Authors/Company\"][n],\n",
    "                boxes_dict[\"Edition/Console\"][n],\n",
    "                boxes_dict[\"Price_std_or_min\"][n],\n",
    "                boxes_dict[\"Max_prices\"][n]]\n",
    "\n",
    "        return boxes_dict, dict_rows\n",
    "    else:\n",
    "        return boxes_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIxHGZeHJJau"
   },
   "source": [
    "##mx dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6BytG0_NY0z"
   },
   "outputs": [],
   "source": [
    "mx_dict = {\n",
    "    'top_food_and_drinks':'grocery',\n",
    "    'top_automotive':'automotive',\n",
    "    'top_baby':'baby',\n",
    "    'top_sports':'sports',\n",
    "    'top_amazon_devices':'amazon-devices',\n",
    "    'top_electronics':'electronics',\n",
    "    'top_tools':'tools',\n",
    "    'top_kitchen':'kitchen',\n",
    "    'top_industrial_emp_cience':'industrial',\n",
    "    'top_musical_instruments':'musical-instruments',\n",
    "    'top_toys':'toys',\n",
    "    'top_books':'books',\n",
    "    'top_music':'music',\n",
    "    'top_officeproduct':'officeproduct',\n",
    "    'top_movies':'dvd',\n",
    "    'top_handmade':'handmade',\n",
    "    'top_pet_supplies':'pet-supplies',\n",
    "    'top_fashion':'shoes',\n",
    "    'top_health':'hpc',\n",
    "    'top_software':'software',\n",
    "    'top_kindle':'digital-text',\n",
    "    'top_videogames':'videogames'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkWwKXVsdSCl"
   },
   "source": [
    "##br ditionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oREOQDLudTSn"
   },
   "outputs": [],
   "source": [
    "br_dict = {\n",
    "    'top_amazon_devices':'amazon-devices',\n",
    "    'top_automotivo': 'automotive',\n",
    "    'top_food_and_drinks':'grocery',\n",
    "    'top_baby':'baby-products',\n",
    "    'top_sports':'sports',\n",
    "    'top_electronics':'electronics',\n",
    "    'top_tools':'hi',\n",
    "    'top_kitchen':'kitchen',\n",
    "    'top_toys':'toys',\n",
    "    'top_books':'books',\n",
    "    'top_music':'music',\n",
    "    'top_officeproduct':'office',\n",
    "    'top_movies':'dvd',\n",
    "    'top_pet_supplies':'pet-products',\n",
    "    'top_fashion':'fashion',\n",
    "    'top_health':'hpc',\n",
    "    'top_software':'computers',\n",
    "    'top_kindle':'digital-text',\n",
    "    'top_videogames':'videogames',\n",
    "    'top_apps':'mobile-apps',\n",
    "    'top_beauty':'beauty',\n",
    "    'top_home':'home',\n",
    "    'top_home_appliances':'appliances',\n",
    "    'top_pool_garden':'lawn-and-garden',\n",
    "    'top_furniture':'furniture'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KyGrOIHJPTM"
   },
   "source": [
    "##domain dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_in6wP3hLCW"
   },
   "outputs": [],
   "source": [
    "domain_dict = {'Mexico' : 'mx',\n",
    "               'Brazil' : 'br'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEgX02X0JRbj"
   },
   "source": [
    "##domain path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yW-MVOLRhAZ5"
   },
   "outputs": [],
   "source": [
    "domain_path = '4SS/4SS_db/testing/{}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdq7sVD9f796"
   },
   "source": [
    "# Test Amazon Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBpUTFbaf_kT"
   },
   "outputs": [],
   "source": [
    "def test_scrap_amazon(country):\n",
    "    if country == 'mx':\n",
    "        categories_dict = mx_dict\n",
    "        n_spaces = 45\n",
    "\n",
    "    elif country == 'br':\n",
    "        categories_dict = br_dict\n",
    "        n_spaces = 47\n",
    "\n",
    "    log_status = [None]*n_spaces\n",
    "    log_date = [None]*n_spaces\n",
    "    log_domain = [None]*n_spaces\n",
    "    log_category = [None]*n_spaces\n",
    "    n_header = 0\n",
    "\n",
    "    #Category that is scraping\n",
    "    key = 'top_music'\n",
    "    category = categories_dict[key]\n",
    "    \n",
    "    #The main scrap\n",
    "    url = f'https://www.amazon.com.{country}/gp/bestsellers/{category}/ref=zg_bs_nav_0'\n",
    "    soup, status = extract_soup(url,n_header, preview=False)\n",
    "\n",
    "    if status == 503:\n",
    "        while status == 503:\n",
    "            time.sleep(1)\n",
    "            soup, status = extract_soup(url, preview=False)\n",
    "            log_status[n_log] = status\n",
    "            log_date[n_log] = datetime.now()\n",
    "\n",
    "    if status ==200:\n",
    "        boxes = top_amazon_boxes(soup)\n",
    "        amz_key_top_boxes, amz_rows = scrap_boxes(boxes, country, dict_to_rows=True)\n",
    "\n",
    "        for key in amz_key_top_boxes:\n",
    "            print(key, end=' | ')\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "        for row in amz_rows:\n",
    "            for idx in range(len(row)):        \n",
    "                print(row[idx], end=' | ')\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wePEt6W2JVjo"
   },
   "source": [
    "#scrap amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CUMnPywRNfuM"
   },
   "outputs": [],
   "source": [
    "def scrap_amazon():\n",
    "    MX__COL_tz = 'America/Mexico_City'\n",
    "    timezone_MXCOL = timezone(MX__COL_tz) \n",
    "\n",
    "    Start_datetime = datetime.now(timezone_MXCOL)\n",
    "    Start_datetime = Start_datetime.strftime('%Y-%m-%d %H_%M')\n",
    "    centered_date = centered_string(Start_datetime)\n",
    "\n",
    "    n_log = 0\n",
    "    country_count = 0\n",
    "\n",
    "    print(f'      Log Date    | Country |        category       | header | status | Loaded |')\n",
    "\n",
    "    for country_name in domain_dict:\n",
    "        country = domain_dict[country_name]\n",
    "\n",
    "        if country == 'mx':\n",
    "            categories_dict = mx_dict\n",
    "            n_spaces = 45\n",
    "\n",
    "        elif country == 'br':\n",
    "            categories_dict = br_dict\n",
    "            n_spaces = 47\n",
    "\n",
    "        log_status = [None]*n_spaces\n",
    "        log_date = [None]*n_spaces\n",
    "        log_domain = [None]*n_spaces\n",
    "        log_category = [None]*n_spaces\n",
    "        n_header = 0\n",
    "\n",
    "        country_count = country_count + 1\n",
    "\n",
    "        if country_count > 1:\n",
    "            print(' ---------------- | ------- | --------------------- | ------ | ------ | ------ |')\n",
    "\n",
    "\n",
    "        for key in categories_dict:\n",
    "            #Date-time\n",
    "            log_datetime = datetime.now(timezone_MXCOL)\n",
    "            log_date[n_log] = log_datetime.strftime('%Y-%m-%d %H:%M')\n",
    "            print(f' {log_date[n_log]}', end=' | ')\n",
    "            \n",
    "            #Country where you are at\n",
    "            log_domain[n_log] = country\n",
    "            print(f'   {log_domain[n_log]}  ', end=' | ')\n",
    "\n",
    "            #Category that is scraping\n",
    "            category = categories_dict[key]\n",
    "            log_category[n_log] = category\n",
    "            centered_category = centered_string(log_category[n_log])\n",
    "            print(centered_category, end=' | ')\n",
    "            \n",
    "            #The main scrap\n",
    "            url = f'https://www.amazon.com.{country}/gp/bestsellers/{category}/ref=zg_bs_nav_0'\n",
    "\n",
    "            \n",
    "            print(f'  {n_header}   ', end=' | ')\n",
    "\n",
    "            soup, status = extract_soup(url,n_header, preview=False)\n",
    "            n_header += 1\n",
    "            if n_header > 2:\n",
    "                n_header = 0\n",
    "\n",
    "            if status == 503:\n",
    "                while status == 503:\n",
    "                    time.sleep(1)\n",
    "                    soup, status = extract_soup(url, preview=False)\n",
    "                    log_status[n_log] = status\n",
    "                    log_date[n_log] = datetime.now()\n",
    "\n",
    "            if status ==200:\n",
    "                log_status[n_log] = status\n",
    "                print(f'  {log_status[n_log]} ', end=' | ')\n",
    "                \n",
    "                boxes = top_amazon_boxes(soup)\n",
    "                amz_key_top_boxes, amz_rows = scrap_boxes(boxes, country, dict_to_rows=True)\n",
    "                add_df = pd.DataFrame.from_dict(amz_key_top_boxes)\n",
    "\n",
    "                #Updating Parquet files\n",
    "                bdb = f'/{country}-{category}.parquet'\n",
    "                dir_parquet_maindb = f'/content/drive/My Drive/Colab Notebooks/4SS/4SS_db/testing/Masters/{country}/parquet/{bdb}'\n",
    "                main_df = pd.read_parquet(dir_parquet_maindb)\n",
    "                main_df = pd.concat([main_df, add_df])\n",
    "                #case of NaN values\n",
    "                main_df['Rank'] = pd.to_numeric(main_df['Rank'], downcast='float')\n",
    "\n",
    "                parquet_file = f'/{country}-{category}.parquet'\n",
    "                dir_parquet_maindb = f'/content/drive/My Drive/Colab Notebooks/4SS/4SS_db/testing/Masters/{country}/parquet/{parquet_file}'\n",
    "                #Updating  file\n",
    "                main_df.to_parquet(dir_parquet_maindb)\n",
    "\n",
    "                excel_file = f'/{country}-{category}.xlsx'\n",
    "                dir_excel_maindb = f'/content/drive/My Drive/Colab Notebooks/4SS/4SS_db/testing/Masters/{country}/excel/{excel_file}'\n",
    "                #For the testing\n",
    "                main_df.to_excel(dir_excel_maindb, index=False)\n",
    "\n",
    "                #Updating csv files\n",
    "                csv_file = f'/{country}-{category}.csv'\n",
    "                dir_csv_maindb = f'/content/drive/My Drive/Colab Notebooks/4SS/4SS_db/testing/Masters/{country}/csv/{csv_file}'\n",
    "\n",
    "                print('  Yes  |')\n",
    "\n",
    "            else:\n",
    "                log_status[n_log] == status\n",
    "                print(f'  {log_status[n_log]} ', end=' | ')\n",
    "                \n",
    "                log_date[n_log] = datetime.now()\n",
    "                print(' +.No.+ |')\n",
    "\n",
    "            n_log = n_log + 1\n",
    "\n",
    "    log_dict = {\n",
    "        'log_date' : log_date,\n",
    "        'category' : log_category,\n",
    "        'country' : log_domain,\n",
    "        'status' : log_status\n",
    "    }\n",
    "\n",
    "\n",
    "    dict_rows= [\n",
    "        log_date,\n",
    "        log_category,\n",
    "        log_domain,\n",
    "        log_status]\n",
    "\n",
    "    log_file = f'/master_db_logs.csv'\n",
    "    dir_log_testing = f'/content/drive/My Drive/Colab Notebooks/4SS/4SS_db/testing/Masters/{log_file}'\n",
    "\n",
    "    try:\n",
    "        update_csv(dir_log_testing, log_rows, sep='|')\n",
    "    except:\n",
    "        log_df = pd.DataFrame.from_dict(log_dict)\n",
    "        log_df.to_csv(dir_log_testing, index = False)\n",
    "\n",
    "\n",
    "\n",
    "    log_print = \"\"\"\n",
    "                                ----------------------\n",
    "                                |   Log file loaded   |\n",
    "                                ----------------------\n",
    "\n",
    "    \"\"\"\n",
    "    print(log_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpqJXhePJbnt"
   },
   "source": [
    "#Scrap Amazon n times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVWqk3m6xFdS"
   },
   "outputs": [],
   "source": [
    "def scrap_amazon_times(n_times, minutes):\n",
    "    counter = 0\n",
    "    for t in range(n_times):\n",
    "        for min in range(minutes):\n",
    "            print(min, end='')\n",
    "            line_jump = 0\n",
    "            for wait in range(3):\n",
    "                time.sleep(15)\n",
    "                print('.', end='')\n",
    "                if (min > 0) and (min % 20 == 0) and (line_jump == 0):\n",
    "                    line_jump = 1\n",
    "                    print('\\n')\n",
    "        \n",
    "        print(f'\\nScrap No: {counter + 1}\\n')\n",
    "        scrap_amazon()\n",
    "        counter = counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNrorwGPJePs"
   },
   "source": [
    "#Wait this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iubbUYkdvW16"
   },
   "outputs": [],
   "source": [
    "#time.sleep(60*30)\n",
    "#scrap_amazon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yU4rNbZpJgFT"
   },
   "source": [
    "#Scrap n times in t time or just one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUp2kjgnLqxY"
   },
   "outputs": [],
   "source": [
    "test = False\n",
    "if test == True:\n",
    "    test_scrap_amazon(country='mx')\n",
    "else:\n",
    "    scrap_amazon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mLGJwY9tbGcL",
    "outputId": "b3756261-b0e0-4751-8876-7b62b6faeed6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...1...2...3...4...5...6...7...8...9...10...11...12...13...14...15...16...17...18...19...20.\n",
      "\n",
      "..21...22...23...24...25...26...27...28...29...30...31...32...33...34...35...36...37...38...39...40.\n",
      "\n",
      "..41...42...43...44...45...46...47...48...49...50...51...52...53...54...55...56...57...58...59...\n",
      "Scrap No: 1\n",
      "\n",
      "      Log Date    | Country |        category       | header | status | Loaded |\n",
      " 2020-10-01 21:40 |    mx   |        grocery        |   0    |   200  |   Yes  |\n",
      " 2020-10-01 21:40 |    mx   |       automotive      |   1    |   200  |   Yes  |\n",
      " 2020-10-01 21:40 |    mx   |          baby         |   2    |   200  |   Yes  |\n",
      " 2020-10-01 21:40 |    mx   |         sports        |   0    |   200  |   Yes  |\n",
      " 2020-10-01 21:41 |    mx   |     amazon-devices    |   1    |   200  |   Yes  |\n",
      " 2020-10-01 21:41 |    mx   |      electronics      |   2    |   200  |   Yes  |\n",
      " 2020-10-01 21:41 |    mx   |         tools         |   0    |   200  |   Yes  |\n",
      " 2020-10-01 21:41 |    mx   |        kitchen        |   1    |   200  |   Yes  |\n",
      " 2020-10-01 21:41 |    mx   |       industrial      |   2    |   200  |   Yes  |\n",
      " 2020-10-01 21:41 |    mx   |  musical-instruments  |   0    |   200  |   Yes  |\n",
      " 2020-10-01 21:42 |    mx   |          toys         |   1    |   200  |   Yes  |\n",
      " 2020-10-01 21:42 |    mx   |         books         |   2    |   200  |   Yes  |\n",
      " 2020-10-01 21:42 |    mx   |         music         |   0    |   200  |   Yes  |\n",
      " 2020-10-01 21:42 |    mx   |     officeproduct     |   1    |   200  |   Yes  |\n",
      " 2020-10-01 21:42 |    mx   |          dvd          |   2    |   200  |   Yes  |\n",
      " 2020-10-01 21:42 |    mx   |        handmade       |   0    |   200  |   Yes  |\n",
      " 2020-10-01 21:42 |    mx   |      pet-supplies     |   1    |   200  |   Yes  |\n",
      " 2020-10-01 21:43 |    mx   |         shoes         |   2    |   200  |   Yes  |\n",
      " 2020-10-01 21:43 |    mx   |          hpc          |   0    |   200  |   Yes  |\n",
      " 2020-10-01 21:43 |    mx   |        software       |   1    |   200  |   Yes  |\n",
      " 2020-10-01 21:43 |    mx   |      digital-text     |   2    |   200  |   Yes  |\n",
      " 2020-10-01 21:43 |    mx   |       videogames      |   0    |   200  |   Yes  |\n",
      " ---------------- | ------- | --------------------- | ------ | ------ | ------ |\n",
      " 2020-10-01 21:43 |    br   |     amazon-devices    |   0    |   200  |   Yes  |\n",
      " 2020-10-01 21:43 |    br   |       automotive      |   1    |   200  |   Yes  |\n",
      " 2020-10-01 21:44 |    br   |        grocery        |   2    |   200  |   Yes  |\n",
      " 2020-10-01 21:44 |    br   |     baby-products     |   0    |   200  |   Yes  |\n",
      " 2020-10-01 21:44 |    br   |         sports        |   1    |   200  |   Yes  |\n",
      " 2020-10-01 21:44 |    br   |      electronics      |   2    |   200  |   Yes  |\n",
      " 2020-10-01 21:44 |    br   |           hi          |   0    |   200  |   Yes  |\n",
      " 2020-10-01 21:44 |    br   |        kitchen        |   1    |   200  |   Yes  |\n",
      " 2020-10-01 21:44 |    br   |          toys         |   2    |   200  |   Yes  |\n",
      " 2020-10-01 21:45 |    br   |         books         |   0    |   200  |   Yes  |\n",
      " 2020-10-01 21:45 |    br   |         music         |   1    |   200  |   Yes  |\n",
      " 2020-10-01 21:45 |    br   |         office        |   2    |   200  |   Yes  |\n",
      " 2020-10-01 21:45 |    br   |          dvd          |   0    |   200  |   Yes  |\n",
      " 2020-10-01 21:45 |    br   |      pet-products     |   1    |   200  |   Yes  |\n",
      " 2020-10-01 21:45 |    br   |        fashion        |   2    |   200  |   Yes  |\n",
      " 2020-10-01 21:45 |    br   |          hpc          |   0    |   200  |   Yes  |\n",
      " 2020-10-01 21:46 |    br   |       computers       |   1    |   200  |   Yes  |\n",
      " 2020-10-01 21:46 |    br   |      digital-text     |   2    |   200  |   Yes  |\n",
      " 2020-10-01 21:46 |    br   |       videogames      |   0    |   200  |   Yes  |\n",
      " 2020-10-01 21:46 |    br   |      mobile-apps      |   1    |   200  |   Yes  |\n",
      " 2020-10-01 21:46 |    br   |         beauty        |   2    |   200  |   Yes  |\n",
      " 2020-10-01 21:46 |    br   |          home         |   0    |   200  |   Yes  |\n",
      " 2020-10-01 21:46 |    br   |       appliances      |   1    |   200  |   Yes  |\n",
      " 2020-10-01 21:47 |    br   |    lawn-and-garden    |   2    |   200  |   Yes  |\n",
      " 2020-10-01 21:47 |    br   |       furniture       |   0    |   200  |   Yes  |\n",
      "\n",
      "                                ----------------------\n",
      "                                |   Log file loaded   |\n",
      "                                ----------------------\n",
      "\n",
      "    \n",
      "0...1...2...3...4...5...6...7...8...9...10...11...12...13...14...15...16...17...18...19...20.\n",
      "\n",
      "..21...22...23...24...25...26...27...28...29...30...31...32...33...34...35...36...37...38...39...40.\n",
      "\n",
      "..41...42...43...44...45...46...47...48...49...50...51...52...53...54...55...56...57...58...59...\n",
      "Scrap No: 2\n",
      "\n",
      "      Log Date    | Country |        category       | header | status | Loaded |\n",
      " 2020-10-01 22:32 |    mx   |        grocery        |   0    |   200  |   Yes  |\n",
      " 2020-10-01 22:32 |    mx   |       automotive      |   1    |   200  |   Yes  |\n",
      " 2020-10-01 22:32 |    mx   |          baby         |   2    |   200  |   Yes  |\n",
      " 2020-10-01 22:32 |    mx   |         sports        |   0    |   200  |   Yes  |\n",
      " 2020-10-01 22:32 |    mx   |     amazon-devices    |   1    |   200  |   Yes  |\n",
      " 2020-10-01 22:33 |    mx   |      electronics      |   2    |   200  |   Yes  |\n",
      " 2020-10-01 22:33 |    mx   |         tools         |   0    |   200  |   Yes  |\n",
      " 2020-10-01 22:33 |    mx   |        kitchen        |   1    |   200  |   Yes  |\n",
      " 2020-10-01 22:33 |    mx   |       industrial      |   2    |   200  |   Yes  |\n",
      " 2020-10-01 22:33 |    mx   |  musical-instruments  |   0    |   200  |   Yes  |\n",
      " 2020-10-01 22:33 |    mx   |          toys         |   1    |   200  |   Yes  |\n",
      " 2020-10-01 22:33 |    mx   |         books         |   2    |   200  |   Yes  |\n",
      " 2020-10-01 22:34 |    mx   |         music         |   0    |   200  |   Yes  |\n",
      " 2020-10-01 22:34 |    mx   |     officeproduct     |   1    |   200  |   Yes  |\n",
      " 2020-10-01 22:34 |    mx   |          dvd          |   2    |   200  |   Yes  |\n",
      " 2020-10-01 22:34 |    mx   |        handmade       |   0    |   200  |   Yes  |\n",
      " 2020-10-01 22:34 |    mx   |      pet-supplies     |   1    |   200  |   Yes  |\n",
      " 2020-10-01 22:34 |    mx   |         shoes         |   2    |   200  |   Yes  |\n",
      " 2020-10-01 22:35 |    mx   |          hpc          |   0    |   200  |   Yes  |\n",
      " 2020-10-01 22:35 |    mx   |        software       |   1    |   200  |   Yes  |\n",
      " 2020-10-01 22:35 |    mx   |      digital-text     |   2    |   200  |   Yes  |\n",
      " 2020-10-01 22:35 |    mx   |       videogames      |   0    |   200  |   Yes  |\n",
      " ---------------- | ------- | --------------------- | ------ | ------ | ------ |\n",
      " 2020-10-01 22:35 |    br   |     amazon-devices    |   0    |   200  |   Yes  |\n",
      " 2020-10-01 22:35 |    br   |       automotive      |   1    |   200  |   Yes  |\n",
      " 2020-10-01 22:35 |    br   |        grocery        |   2    |   200  |   Yes  |\n",
      " 2020-10-01 22:35 |    br   |     baby-products     |   0    |   200  |   Yes  |\n",
      " 2020-10-01 22:36 |    br   |         sports        |   1    |   200  |   Yes  |\n",
      " 2020-10-01 22:36 |    br   |      electronics      |   2    |   200  |   Yes  |\n",
      " 2020-10-01 22:36 |    br   |           hi          |   0    |   200  |   Yes  |\n",
      " 2020-10-01 22:36 |    br   |        kitchen        |   1    |   200  |   Yes  |\n",
      " 2020-10-01 22:36 |    br   |          toys         |   2    |   200  |   Yes  |\n",
      " 2020-10-01 22:36 |    br   |         books         |   0    |   200  |   Yes  |\n",
      " 2020-10-01 22:36 |    br   |         music         |   1    |   200  |   Yes  |\n",
      " 2020-10-01 22:37 |    br   |         office        |   2    |   200  |   Yes  |\n",
      " 2020-10-01 22:37 |    br   |          dvd          |   0    |   200  |   Yes  |\n",
      " 2020-10-01 22:37 |    br   |      pet-products     |   1    |   200  |   Yes  |\n",
      " 2020-10-01 22:37 |    br   |        fashion        |   2    |   200  |   Yes  |\n",
      " 2020-10-01 22:37 |    br   |          hpc          |   0    |   200  |   Yes  |\n",
      " 2020-10-01 22:37 |    br   |       computers       |   1    |   200  |   Yes  |\n",
      " 2020-10-01 22:37 |    br   |      digital-text     |   2    |   200  |   Yes  |\n",
      " 2020-10-01 22:38 |    br   |       videogames      |   0    |   200  |   Yes  |\n",
      " 2020-10-01 22:38 |    br   |      mobile-apps      |   1    |   200  |   Yes  |\n",
      " 2020-10-01 22:38 |    br   |         beauty        |   2    |   200  |   Yes  |\n",
      " 2020-10-01 22:38 |    br   |          home         |   0    |   200  |   Yes  |\n",
      " 2020-10-01 22:38 |    br   |       appliances      |   1    |   200  |   Yes  |\n",
      " 2020-10-01 22:38 |    br   |    lawn-and-garden    |   2    |   200  |   Yes  |\n",
      " 2020-10-01 22:38 |    br   |       furniture       |   0    |   200  |   Yes  |\n",
      "\n",
      "                                ----------------------\n",
      "                                |   Log file loaded   |\n",
      "                                ----------------------\n",
      "\n",
      "    \n",
      "0...1...2...3...4...5...6...7...8...9...10...11...12...13...14...15...16...17...18...19"
     ]
    }
   ],
   "source": [
    "case = 'steps'\n",
    "#case = 'normal'\n",
    "\n",
    "if case == 'steps':\n",
    "    scrap_amazon_times(n_times= 11, minutes= 60)\n",
    "else:\n",
    "    scrap_amazon()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "amazon_scraper_boxes_df.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
